% ************************** Thesis Abstract *****************************
% Use `abstract' as an option in the document class to print only the titlepage and the abstract.
\begin{abstract}


Recognizing event in unconstrained video is one of the most important tasks in multimedia retrieval. It has potential for many applications such as video indexing, searching, and event recounting. However it is a challenging task due to the large content variation and uncontrolled capturing conditions. This leads to the fact that these videos often contain irrelevant information to the event of interest. The straightforward way to solve this problem is decomposing the video into smaller segments and building the event detectors from these segment representations. This dissertation implements this vision in three complementary approaches. These approaches range from simple solutions, which can only detect event, to more complex ones, which can provide evidences for event detection. 

In the first approach, we analyze the limitation of the video-based approaches and demonstrate the effectiveness of using segment-based representation. In the traditional video-based approaches, local features are extracted from the entire video and then aggregated to form the final video representation. However, this video-based representation is ineffective when used for realistic videos because the video length can be very different and the clues for determining an event may happen in only a small segment of the entire video. To handle this problem, we propose to divide the original videos into segments for feature extraction and classification, while still keeping the evaluation at the video level. We call this solution segment-based approach for video representation. In this research, we carry an excessive experiments to confirm the correctness of the direction. 

The second approach handles the aforementioned problem by proposing a new pooling strategy for feature aggregation. We consider a video as a layered structure where the lowest layer are frames, the top layer is the entire video, and the middle layers are the sequences of consecutive frames or the concatenation of lower layers. While it is easy to find local discriminative features in video from lower layers, it is non-trivial to aggregate these features into a discriminative video representation. In literature, people often use sum pooling to obtain reasonable recognition performance on artificial videos. However, the sum pooling technique does not work well on complex videos because the region of interests may reside within some middle layers. In this approach, we leverage the layered structure of video to propose a new pooling method, named sum-max video pooling, to handle this problem. Basically, we apply sum pooling at the low layer representation while using max pooling at the high layer representation. Sum pooling is used to keep sufficient relevant features at the low layer, while max pooling is used to retrieve the most relevant features at the high layer, therefore it can discard irrelevant features in the final video representation. 
	
In the third approach, we focus on learning the key segments for video representation. In fact, a complex event can be recognized by observing necessary evidences. It is not easy to locate supportive evidences because they can happen anywhere in a video. A straightforward solution is to decompose the video into several segments and search for the evidences in each segment. This approach is based on the assumption that segment annotation can be assigned from its video label. However, this is a weak assumption because the importance of each segment is not considered. On the other hand, the importance of a segment to an event can be obtained by matching its detected concepts against the evidential description of that event. Leveraging this prior knowledge, we propose a new method, Event-driven Multiple Instance Learning (EDMIL), to learn the key evidences for event detection. We treat each segment as an instance and quantize the instance-event similarity into different levels of relatedness. Then the instance label is learned by jointly optimizing the instance classifier and its related level. Finally the optimal instance classifiers are used to detect event.

We verify the effectiveness of our approaches on the large scale TRECVID Multimedia Event Detection 2010, 2011 and 2012 datasets. Our approaches not only detect event, but also provide evidences for event detection. Compared to other segment-based approaches, our solutions achieve significant improvements. 

\end{abstract}
