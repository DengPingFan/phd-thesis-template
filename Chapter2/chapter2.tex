\chapter{Background}
\label{chapter2}
%\setlength{\epigraphrule}{0pt}
\epigraph{\textit{We can draw lessons from the past, but we cannot live in it.}}{ -- Lyndon B. Johnson}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi


\section{TRECVID Multimedia Event Detection}

As introduced in Chapter \ref{chapter1}, complex event recognition is an important computer vision research with many potential applications. In 2010 TRECVID community has proposed a new task, named ``Multimedia Event Detection'' \cite{over2011trecvid} to advance the research and development in this area. The ultimate purpose of this task is to collect technologies for building a computer system that can quickly search for a particular event over a large video collection in a reasonable response time. \index{trecvid} \index{multimedia event detection}
 
The task is defined as follows: ``Given an event kit, find all clips that contain the event in a video collection'' \cite{over2011trecvid}. The event kit provides the event definitions along with some example videos of each event. At first, MED task defines an event:\textit{is a complex activity occurring at a specific place and time; involves people interacting with other people and/or objects; consists of a number of human actions, processes, and activities that are loosely or tightly organized and that have significant temporal and semantic relationships to the overarching activity; and is directly observable}. 

For a specific event of interest, a textual description is also provided to help developers generate the event search query. This textual description consists of following information: event name, event definition, event explication and evidential description. The event name is a mnemonic title of that event. The event definition provides a short definition of that event. Event explication is a long description which explains ambiguous terminologies in the event definition. Finally, the evidential description summarizes an event with its characteristics such as scene, object/people, activities and audio information. Table \ref{c2_eventkit} shows textual description of an event in MED task.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}

\begin{table}
	\centering	
	\caption{Textual description for event ``Attempting a board trick''}
	\renewcommand{\arraystretch}{1.5}	
	\begin{tabular}{|l|l|}
		\toprule
		Event name             & Attempting a board trick                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\ \midrule
		Definition             & \begin{tabular}[c]{@{}l@{}}One or more people attempt to do a trick on a skateboard, snowboard, \\ surfboard, or other boardsport board.\end{tabular}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\ \midrule
		Explication            & \begin{tabular}[c]{@{}l@{}}Board sports are sports where a person stands, sits, or lays on a board \\ and moves and controls the board. Tricks consist of intentional motions\\ made with the board that are not simply slowing down/stopping the board \\ or steering the board as it moves. Steering around obstacles or steering a \\ board off of a jump and landing on the ground are not considered tricks in \\ and of themselves.\\ Common tricks involve actions like sliding the board along the top of an\\ object (e.g. a swimming pool rim or railing), jumping from the ground or\\ the surface of water into the air, and spinning or flipping in the air.\end{tabular} \\ \midrule
		\begin{tabular}[c]{@{}l@{}}Evidential \\ description\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{scene}: outside, often in a skate park.\\ \textbf{objects/people}: skateboard, snowboard, surfboard, ramps, rails, safety\\ gear, crowds.\\ \textbf{activities}: standing, sitting or laying on the board; jumping with the board; \\ flipping the board and landing on it; spinning the board; sliding the board \\ across various objects.\\ \textbf{audio}: sounds of board hitting surface during trick; crowd cheering.\end{tabular}                                       \\ \bottomrule
	\end{tabular}
	\label{c2_eventkit}
\end{table}



\section{Dataset}

The TRECVID MED organizer also provides a standard benchmark for participants to evaluate their methods \cite{over2011trecvid}. In the pilot task (MED 2010), there are only three events that are being tested\footnote{http://www.nist.gov/itl/iad/mig/med10.cfm}. These events are the following:
(1) ``\textit{Assembling a shelter}'': one or more people construct a temporary or semi-permanent shelter for humans that could provide protection from the elements.
(2) ``\textit{Batting a run in}'': within a single play during a baseballtype game, a batter hits a ball and one or more runners (possibly including the batter) scores a run.
And (3) ``\textit{Making a cake}'': One or more people make a cake. This collection is divided into two subsets including 1,744 videos for training and 1,724 videos for testing.

Since 2011, the number of test events has been increasing. New tested events as well as tested videos are added every year. For example, there are 5 training events (E001-E005) and 10 testing events (E006-E015) in MED 2011\footnote{http://www.nist.gov/itl/iad/mig/med11.cfm}. The number for MED 2012 is 20 testing events (E006-E015, E021-E030)\footnote{http://www.nist.gov/itl/iad/mig/med12.cfm}. These events are also kept in MED 2013 but more testing videos are added. In MED 2014, a different test set with 10 new events are introduced (E021-E040). List of all event names up to TRECVID MED 2014 can be found in Table \ref{c2_eventlist}. Since MED 2012, the evaluation set which contains around 98,000 test videos has been frozen. This collection is blind to all participants, which means they are not allowed to analyze these videos when tuning their systems. 

Since MED 2014, the evaluation set has been doubled by adding around 100,000 test videos. An overview of all MED video collections is shown in Table \ref{c2_dataset}. To the best of our knowledge, this is largest video dataset for event detection purpose. Because since MED 2012, the evaluation dataset has been frozen, most researchers conducts experiments on MED2010, MED2011 and MED2012 dataset \cite{tang2012learning,vahdat2013compositional,lai2014recognizing,lai2014video}. The detail information of these datasets can be seen in Table \ref{c2_exp_dataset}.


\begin{table}[h]
	\centering
	\caption{Number of videos and video hours in the MED dataset up to 2014 \cite{over2014trecvid}.}
	\begin{tabular}{@{}|c|l|c|c|@{}}
		\toprule
		\multicolumn{2}{|c|}{Set}                                                                         & Number of video clips & Video duration (hours) \\ \midrule
		\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Development\\ Data\end{tabular}}    & RESEARCH         & 10,000                & 314                    \\ \cmidrule(l){2-4} 
		& 10 Event Kits    & 1,400                 & 74                     \\ \cmidrule(l){2-4} 
		& Transcription    & 1,500                 & 45                     \\ \midrule
		\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Event\\ Training Data\end{tabular}} & Event Background & 5,000                 & 146                    \\ \cmidrule(l){2-4} 
		& 40 Event Kits    & 6,000                 & 270                    \\ \midrule
		\multirow{2}{*}{Test Data}                                                     & MEDTest          & 27,000                & 849                    \\ \cmidrule(l){2-4} 
		& KindredTest      & 14,500                & 687                    \\ \midrule
		\multirow{2}{*}{Evaluation Data}                                               & MED14Eval-Full   & 198,000               & 7,580                  \\ \cmidrule(l){2-4} 
		& MED14Eval-Sub    & 33,000                & 1,244                  \\ \midrule
		\multicolumn{2}{|c|}{Total}                                                                       & 244,000               & 9,911                  \\ \bottomrule
	\end{tabular}
	\label{c2_dataset}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[h]
		\centering
		\caption{Detail information of MED2010, MED2011 and MED2012 dataset.}
	\begin{tabular}{@{}|c|c|c|c|c|c|@{}}
		\toprule
		Dataset & No. Event & No. Train Videos & No. Test Videos & Total Videos & Total Hours \\ \midrule
		MED2010                     & 3         & 1,744            & 1,724           & 3,468        & 110 hours   \\
		MED2011                     & 10        & 1,331            & 31,822          & 33,153       & 1,100 hours \\
		MED2012                     & 25        & 3,878            & 1,938           & 5,816        & 250 hours   \\ \bottomrule
	\end{tabular}
	\label{c2_exp_dataset}
\end{table}

\begin{table}
	\centering
	\caption{List of event names in MED task from 2010-2014.}	
	\renewcommand{\arraystretch}{1.5}		
	\begin{tabular}{@{}|c|l|l|l|@{}}
		\toprule
		\textbf{ID}                & \multicolumn{1}{c|}{\textbf{Event name}} & \textbf{ID} & \textbf{Event name}               \\ \midrule
		E001                       & Attempting a board trick                 & E021        & Attempting a bike trick           \\ \midrule
		E002                       & Feeding an animal                        & E022        & Cleaning an appliance             \\ \midrule
		E003                       & Landing a fish                           & E023        & Dog show                          \\ \midrule
		E004                       & Wedding ceremony                         & E024        & Giving directions to a location   \\ \midrule
		E005                       & Working on a woodworking project         & E025        & Marriage proposal                 \\ \midrule
		E006                       & Birthday party                           & E026        & Renovating a home                 \\ \midrule
		E007                       & Changing a vehicle tire                  & E027        & Rock climbing                     \\ \midrule
		E008                       & Flash mob gathering                      & E028        & Town hall meeting                 \\ \midrule
		E009                       & Getting a vehicle unstuck                & E029        & Winning a race without a vehicle  \\ \midrule
		E010                       & Grooming an animal                       & E030        & Working on a metal crafts project \\ \midrule
		\multicolumn{1}{|l|}{E011} & Making a sandwich                        & E031        & Beekeeping                        \\ \midrule
		\multicolumn{1}{|l|}{E012} & Parade                                   & E032        & Wedding shower                    \\ \midrule
		\multicolumn{1}{|l|}{E013} & Parkour                                  & E033        & Non-motorized vehicle repair      \\ \midrule
		\multicolumn{1}{|l|}{E014} & Repairing an appliance                   & E034        & Fixing musical instrument         \\ \midrule
		\multicolumn{1}{|l|}{E015} & Working on a sewing project              & E035        & Horse riding competition          \\ \midrule
		\multicolumn{1}{|l|}{E016} & Doing homework or studying               & E036        & Felling a tree                    \\ \midrule
		\multicolumn{1}{|l|}{E017} & Hide and seek                            & E037        & Parking a vehicle                 \\ \midrule
		\multicolumn{1}{|l|}{E018} & Hiking                                   & E038        & Playing fetch                     \\ \midrule
		\multicolumn{1}{|l|}{E019} & Installing flooring                      & E039        & Tailgating                        \\ \midrule
		\multicolumn{1}{|l|}{E020} & Writing                                  & E040        & Tuning musical instrument         \\ \bottomrule
	\end{tabular}
	\label{c2_eventlist}
\end{table}

\section{Feature for Event Detection}
\label{c2_sec_med_feature}
\subsection{Image Features}

Image features or still image features can be further classified into global and local features. Global features represent information of the whole frame (or image) while local features focus on some local invariant characteristics.

The most common global feature is color histogram, which is a representation of the distribution of colors in an image. Color histograms can potentially be identical for two images with different object content which happens to share color information. Another popular global feature is GIST \cite{oliva2001modeling}. GIST descriptor describes the dominant spatial structure of a scene in a low dimensional representation.

For local image features, Scale-Invariant Feature Transform (SIFT) has become a standard feature for many image classification tasks. It is proposed by Lowe in \cite{Lowe:2004} to find local maximum of Difference of Gaussians (an approximation of Laplacian of Gaussian) in space and scale. It is a scale invariant local feature, simple and efficient. Other variants of SIFT take into account the keypoint extraction methods, such as Hessian-Laplace interest points detector \cite{mikolajczyk2002affine} and dense sampling \cite{nowak2006sampling}. In both strategies, local features are extracted from multiple scales by using the Gaussian scale space \cite{mikolajczyk2002affine}. In the case of dense sampling, the key points are densely sampled on a grid with a step size of 6 pixels. Once a key point is detected, it is described using the standard SIFT \cite{Lowe:2004}. It is also acknowledged that other descriptors such as RGB-SIFT, Opponent-SIFT, and C-SIFT \cite{burghouts2009performance} can be complementary with the standard SIFT descriptor \cite{yu2014informedia}.

\subsection{Motion Features}
Motion features have been widely developed for various action recognition tasks. Because event video may contain multiple actions, it is reasonable to employ motion information for event detection. Depend on the extraction methods, motion features can be classified into two categories: methods based on interest points and methods based on tracking. 


\textbf{Methods based on interest points}. The Harris3D detector was proposed by Laptev and Linderberg~\cite{Laptev03space-timeinterest}. This is an extension of Harris2D detector which is used to detect corners in image. Generally the proposed detector will detect points which have significant change in both spatial and temporal direction. First, the spatial temporal second-moment matrix averaged using a Gaussian weighting function \textit{g} at spatial scale $\sigma_{i}$ and temporal scale $\tau_{i}$ is defined as below:

\begin{equation}
	\mu = g(.; \sigma_{i}^{2},\tau_{i}^{2},)* \left( \begin{array}{ccc}L_{x}^{2} & L_{x}L_{y} & L_{x}L_{t} 
		\\L_{x}L_{y} & L_{y}^{2} & L_{y}L_{t} \\ L_{x}L_{t} & L_{y}L_{t} & L_{t}^{2}\end{array} \right)
\end{equation}

Second, a new response function is proposed to measure the motion in 3D:

\begin{equation}
	H = \mbox{det}(\mu) + k\, \mbox{trace}^{3}(\mu),
\end{equation}

Finally, interest points are those which maximize the response function H. To do this, the authors use a corresponding eigenvalue problem. The response function H can be rewrote as below:

\begin{equation}
	H = \lambda_{1}\lambda_{2}\lambda_{3} - k\, (\lambda_{1}+\lambda_{2}+\lambda_{3})^{3}
\end{equation}

Interest points are those have large eigenvalue $\lambda_{1},\lambda_{2},\lambda_{3}$ which means large variation in both spatial and temporal domain.


The Cuboid detector is based on temporal Gabor filters and was proposed by Dollár et al. in~\cite{Dollar05VSPETS}. The response function has the form:

\begin{equation}
	R = (I*g*h_{ev})^{2}+(I*g*h_{od})^{2}
\end{equation}
where g(x,y;$\sigma$) is the 2D spatial Gaussian smoothing kernel, and $h_{ev}$ and $h_{od}$ are a quadrature pair of 1D Gabor filters which are applied temporally.
The Gabor filters are defined by:
\begin{equation}
	h_{ev}(t;\tau,\omega) = -cos(2\pi t\omega)e^{\frac{-t^{2}}{\tau^{2}}}
\end{equation}
\begin{equation}
	h_{od}(t;\tau,\omega) = -sin(2\pi t\omega)e^{\frac{-t^{2}}{\tau^{2}}}
\end{equation}
with $\omega$=4/$\tau$. The two parameters $\sigma$ and $\tau$ of the response function R correspond roughly to the spatial and temporal scale of the detector. Interest points are the local maxima of the response function R.

The Hessian detector was proposed by Willems et al.~\cite{willems2008efficient} as a spatio-temporal extension of the Hessian saliency measure used for blob detection in images. The detector measures the saliency with the determinant of the 3D Hessian matrix. The position and scale of the interest points are simultaneously localized without any iterative procedure. In order to speed up the detector, the authors used approximative box-filter operations on an integral video structure. Each octave is divided into 5 scales, with a ratio between subsequent scales in the range 1.2 -- 1.5 for the inner 3 scales. The determinant of the Hessian is computed over several octaves of both the spatial and temporal scales. A non-maximum suppression algorithm selects joint extrema over space, time and scales: (x, y, t, $\sigma$, $\tau$).
	

\textbf{Methods based on tracking}. Methods based on tracking process the video frame by frame. Trajectories are often extracted after tracking for fixed number of frame length. Different features may different in both the extraction and description method. There are several methods to extract trajectories. In~\cite{Matikainen09ICCV}, a standard Kanade–Lucas–Tomasi (KLT) tracker is used to track features (using "good features to track") over a video. A fixed number of features (typically 100) is initialized for tracking. Features are replaced as necessary when tracks are lost. The output of this tracking is a trace of (x; y) pairs for each feature.

In~\cite{Yuan12PR}, trajectories are generated by tracking dense SIFT points frame by frame. The reason for using dense sampling, according to the author, is to provide sufficient points to group similar motions into meaningful body parts. First, points are sampled on a regular grid spacing with 5 pixels, then each image patch is represented by a SIFT descriptor. The correspondence between key-points in successive frames is established by nearest neighbor distance ratio matching.

Trajectories can also be tracked using dense optical flow. This method proposed by Wang in~\cite{wang:2011:inria-00583818:1,wang2013action}. The trajectories are obtained by tracking densely sampled points using optical flow fields. First, feature points are sampled on a grid spaced by 5 pixels and at multiple scales spaced by a factor of 1/$\sqrt{2}$. Then features are tracked in each scale separately. Each point $P_{t} = (x_{t}, y_{t})$ at frame \textit{t} is tracked to the next frame \textit{t+1} by median filtering in a dense optical flow field $\omega$ = ($u_{t}$, $v_{t}$).

Tracking using KLT can be difficult for initialization step, particularly when the scene contains distracting objects. Moreover, sparse tracking methods like KLT might not capture enough information of a moving object. That is why the dense trajectory approach~\cite{wang:2011:inria-00583818:1} yields better result than sparse trajectory extraction approach. However, tracking object is still challenging in real world environment, where occlusion between moving objects are popular. In that case, the tracker output tends to be noisy. On another hand, matching dense SIFT descriptors is computationally very expensive~\cite{LiuCe09CVPR} and, thus, infeasible for large video datasets. 

\textbf{Trajectory descriptors}. The trajectory descriptors are computed within a space-time volume around the trajectory (see Fig.~\ref{densetrack_descriptor}). The size of the volume is NxN pixels and L frames. To embed structure information in the representation, the volume is subdivided into a spatio-temporal grid of size $n_{\sigma}$ x $n_{\sigma}$ x $n_{\tau}$. The default parameters for our experiments are N = 32, $n_{\sigma}$ = 2, $n_{\tau}$ = 3.

\begin{figure}[!h]
	\centering
	\includegraphics[width=1\textwidth]{densetrack_descriptor.jpg}
	\caption{Illustration of dense trajectory description. Left: Feature points are sampled densely for multiple spatial scales. Middle: Tracking is performed in the corresponding spatial scale over L frames. Right: Trajectory descriptors are based on its shape represented by relative point coordinates as well as appearance and motion information over a local neighborhood of N x N pixels along the trajectory. In order to capture the structure information, the trajectory neighborhood is divided into a spatio-temporal grid of size $n_{\sigma}$ x $n_{\sigma}$ x $n_{\tau}$ \cite{wang:2011:inria-00583818:1}.}
	\label{densetrack_descriptor}
\end{figure}

The HOGHOF~\cite{Laptev03space-timeinterest} descriptor has shown excellent results on a variety of datasets. HOG (histograms of oriented gradients) focuses on static appearance information, whereas HOF (histograms of optical flow) captures the local motion information. HOGHOF descriptors are computed along the dense trajectories (see Fig.~\ref{densetrack_descriptor}). For both HOG and HOF, orientations are quantized into 8 bins using full orientations, with an additional zero bin for HOF (i.e., in total 9 bins). Both descriptors are normalized with their $L_{2}$ norm.

The MBH descriptor is proposed by Dalal et al.~\cite{Dalal06ECCV} for human detection, where derivatives are computed separately
for the horizontal and vertical components of the optical flow. This descriptor encodes the relative motion between pixels (See Fig. ~\ref{mbh_descriptor}).
\begin{figure}[!h]
	\centering
	\includegraphics[width=1\textwidth]{mbh_descriptor.jpg}
	\caption{Illustration of the MBH descriptor. (a,b) Reference images at time t and t+1. (c,d) Computed optical flow, and flow magnitude showing motion boundaries. (e,f) Gradient magnitude of flow field $I_{x}$, $I_{y}$ for image pair (a,b). (g,h) Average MBH descriptor over all training images for flow field $I_{x}$, $I_{y}$ \cite{Dalal06ECCV}. }
	\label{mbh_descriptor}
\end{figure}

The MBH descriptor separates the optical flow field $I_{\omega}$ = ($I_{x}$, $I_{y}$) into its x and y component. Spatial derivatives are computed for each of them and orientation information is quantized into histograms, similarly to the HOG descriptor (have 8-bin histogram for each component). Finally, these two histograms are normalized separately with the $L_{2}$ norm. Since MBH represents the gradient of the optical flow, constant motion information is suppressed and only information about changes in the flow field (i.e., motion boundaries) is kept. This is a simple way to eliminate noise due to background motion. This descriptor yields excellent results when combined with dense trajectory features.

As shown by Wang et al. \cite{wang:2011:inria-00583818:1,wang2013action}, the dense trajectory feature is one of the best for action classification. In particular, it is an efficient way to remove camera motion. Violent scenes of Hollywood movies tend to have a lot of action and different effects. We use the dense trajectory feature to capture this information. Trajectories are obtained by tracking densely sampled points in the optical flow fields. As suggested by Wang \cite{wang:2011:inria-00583818:1,wang2013action}, we use Histogram of Oriented Gradient (HOG), Histogram of Optical Flow (HOF) and Motion Boundary Histogram (MBH) to describe each trajectory. HOG captures the appearance of a moving object, whereas HOF captures its speed. The last descriptor, MBH, captures the boundaries of motion and is good for handling camera motion.

\nomenclature[-hog]{HOG}{Histogram of Oriented Gradient}
\nomenclature[-hof]{HOF}{Histogram of Optical Flow}
\nomenclature[-mbh]{MBH}{Motion Boundary Histogram}
\nomenclature[-mfcc]{MFCC}{Mel-frequency Cepstral Coeffcients}
\nomenclature[-klt]{KLT}{Kanade–Lucas–Tomasi}
\nomenclature[-cnn]{CNN}{Convolutional Neural Network}

\subsection{Audio Features}
We use the popular Mel-frequency Cepstral Coeffcients (MFCC) \cite{rabiner2007introduction} for extracting audio features. We set the window to 25 ms and the step size to 10 ms. 13-dimensional MFCC vectors along with their first and second derivatives are used for representing each audio segment. Raw MFCC features are also encoded using BoW. Note that this configuration was used by the winning teams (AXES/LEAR) of the TRECVID Multimedia Event Detection 2013 \cite{aly2013axes} and THUMOS Challenge 2014 \cite{oneata2014lear}.

We investigated several ways to extract MFCC features from audio channel. These MFCC libraries are used in our evaluation: VoiceBox audio toolkit \cite{voicebox}, Yaafe audio library \cite{mathieu2010yaafe} and the RASTA-PLP library \cite{Ellis05-rastamat}. We found that the RASTA-PLP implementation achieved slightly better performance than others. Moreover, we did not observe significant improvement when changing parameters such as window length and step between successive windows. So we kept using the default setting in the RASTA-PLP implementation.

\subsection{Deep Learning Features}
Deep learning has been drawing a lot of attention after the seminal work of Krizhevsky et al. \cite{krizhevsky2012imagenet}. They proposed a deep learning framework that significantly outperforms previous state-of-the-art methods on the ImageNet benchmark \cite{deng2009imagenet}. This is a variant of multilayer perceptrons (MLP) \cite{rumelhart1985learning}, where a larger number of layers can be incorporated into the network (Fig. \ref{c2_deeplearning}). The success of deep learning is due to the explosion of big data, Convolutional Neural Network (CNN) \cite{lecun1995convolutional,lecun1998gradient}, as well as major improvements for training the network \cite{hinton2006fast,bengio2007greedy}.
\begin{figure}[!h]
	\centering
	\includegraphics[width=1\textwidth]{deeplearning.png}
	\caption{Illustration of deep learning architecture that was used in \cite{krizhevsky2012imagenet}.}
	\label{c2_deeplearning}
\end{figure}

There are basically three ways to apply deep learning for event detection from video. (1)
Using pre-trained deep models to extract video features \cite{tran2014c3d}. Note that if the model was trained on image collection \cite{krizhevsky2012imagenet,chatfield2014return,simonyan2014very,zhou2014learning}, it can be used to extract image features for each sample video frames. Video level features can be obtained by aggregating from all of its keyframe-based featrues. (2) The second approach is to train a deep learning model directly on event video collection. As a result, we can have an end-to-end network for the event detection. However, deep learning techniques for video is still not matured, and training a deep model with a limited number of positive labels might be not effective. (3) The third method is fine-tuning for event detection data on top a pre-trained model. This is the most common approach when applying deep learning to a new application. However, this approach requires the pre-trained model should be trained on a dataset that is similar to video event collection, otherwise, deep learning might not be applicable.
 
In this dissertation, we used the popular DeepCaffe \cite{jia2014caffe} framework to extract keyframe features. We used the pre-trained deep model provided by DeepCaffe. This model was trained on ImageNet 1,000 concepts \cite{deng2009imagenet}. The protocol for training it is described in \cite{jia2014caffe}. As suggested in \cite{krizhevsky2012imagenet}, we selected the last three fully connected layers for the feature representation. The third and second-to-last layers have 4,096 dimensions, while the last layer has 1,000 dimensions corresponding to the 1,000 concept categories in the ImageNet dataset. 

\section{General Framework}

We design a unified framework to evaluate the performance of individual features and their combination (see Figure \ref{med_framework}). We made it flexible in that we can easily test different features. We also designed it in components, i.e., pre-processing, feature extraction, feature encoding, feature classification, and feature fusion, so that each component could be evaluated separately while keeping the others intact. In particular, it consists of the following components.

\begin{figure}
	\centering
	\includegraphics[width=1\textwidth]{framework.pdf}
	\caption{General MED framework}
	\label{med_framework}
\end{figure}

\subsection{Pre-Processing}

The pre-processing component prepares the data for the processing in the other components. First, the video is resized to a width of 320 pixels, and its height is scaled so that the aspect ratio is kept the same. All features are extracted from the resized video. 

To get the image features, keyframes are sampled from the shots at one frames every two seconds. This rate seems to be a good tradeoff between time and accuracy (as suggested in \cite{trecvid10:cuucf}). Blank keyframes, i.e., ones filled with a single color, are removed because they do not contain informative features.

To get the audio features, the audio channel is extracted from the original video and saved as a file in the standard WAV format. The audio features can be extracted from this file.

\subsection{Feature Extraction}

The feature extraction component aims to make a discriminative vector representation for each shot extracted in the pre-processing step. The extraction method depends on what type of feature will be used. To conduct comprehensive evaluations of features for VSD, our framework supports a large variety of features, including global and local visual features. Global features capture the global statistics of each extracted shot. These statistics can be calculated directly from sub regions of a sampled frame and concatenated to form the vector representation for that frame, before being aggregated into the final representation for each shot. It is more complicated to calculate the feature vector representation for local features. The number of local features varies from frame to frame; therefore, it requires a special encoding technique, which will be described in Section \ref{c2_sec_feature_encoding}.

Besides global and local features, our evaluation framework supports a number of other features. In particular, audio features can be extracted from pre-defined temporal windows. The features of each window provide local audio characteristics at that temporal location. This means audio features can be considered as local. Another kind of feature is a mid-level feature made using concept detectors. We use general concepts taken from off-the-shelf datasets ~\cite{deng2009imagenet}. In addition, our framework supports state-of-the-art deep learning features, which are extracted from a pre-trained model. A description of each feature is presented in Section \ref{c2_sec_med_feature}.

\subsection{Feature Encoding}
\label{c2_sec_feature_encoding}

\subsubsection{Bag-of-word model}
As for local features, we use the popular Bag-of-Words (BOW) model to generate a fixed-length representation from local descriptors. This model was initially used to represent text documents \cite{harris1954distributional}, and it was first used to represent images by Csurka et al. \cite{Csurka04visualcategorization}. Its extension to motion and audio features is straightforward \cite{sivic2009efficient,jiang2010columbia}.

We used the experiment setup described in \cite{jiang2010representations} to make our bag-of-words models. We set the codebook size to 1,000, because in \cite{jiang2010representations}, performance did not significantly improve when the larger codebooks were used, and a smaller codebook can significantly reduce the computational time for feature encoding as well as feature learning. In order to train the codebook, we randomly selected 1 million local descriptors and clustered them using the K-means algorithm. The local descriptors were assigned to each codeword in a soft-weighting manner \cite{Jiang:2007:TOB} to improve the discriminative power of the encoded feature.

The main drawback of the bag-of-words model is that it does not incorporate spatial information. The simplest way to overcome this problem is to partition the image into sub-regions and encode local features in each region independently. After that, features from all regions are concatenated into a single feature vector. There are many ways to partition an image into sub-regions. To this end, we follow \cite{jiang2010representations} and \cite{lazebnik2006beyond} and use 2 x 2 and 1 x 3 spatial configurations. We found that these spatial configurations are good trade-offs between performance and computational cost of the high-dimensional feature vector.

\subsubsection{Fisher Vector Encoding}
The Fisher vector (FV) was first used for image classification in \cite{jaakkola1999exploiting}. It has since been used for action recognition, such as in \cite{sun2013large} and \cite{wang2013action}. Fisher vector encoding can be considered to be an extension of Bag-of-words encoding. Unlike a bag of features, the Fisher vector encodes both first- and second-order statistics between the local descriptors and the codebook. As a result, it is much longer than the BoW feature when using the same codebook. 

\index{fisher vector} \index{bag-of-words} 

Different from bag-of-words encoding, which often uses k-means to train the codebook, the Fisher vector often uses the Gaussian Mixture Model (GMM) to encode the relative position of each local descriptor to each mixture center. The relatively large expressiveness of the Fisher vector means it can achieve comparable performance to that of BoW while using a much smaller codebook \cite{sanchez2013image,sun2013large}.

In our experiment, we set the number of Gaussians in the GMM model to K = 256. Then we randomly selected 1,000,000 local descriptors for training the model. As suggested in \cite{perronnin2010improving}, it is better to reduce the local feature dimension by using Principal Component Analysis (PCA). The normalization of the output feature is also very important. Following the recommendation in \cite{perronnin2010improving}, we applied power normalization with $\alpha=0.5$ followed by L2-normalization to the Fisher vector.

\nomenclature[-fv]{FV}{Fisher Vector}
\nomenclature[-pca]{PCA}{Principal Component Analysis}
\nomenclature[-gmm]{GMM}{Gaussian Mixture Model}
\nomenclature[-bow]{BOW}{Bag-of-Words}
\nomenclature[-svm]{SVM}{Support Vector Machines}

\subsection{Learning}

Support Vector Machine (SVM) is a standard machine learning algorithm for image and action recognition tasks. Therefore, we also use it in our experiments. To this end, we use the LibSVM \cite{chang2011libsvm} for training and testing. Because we often deal with event collection with more than two events, we simply adopt the one-vs.-rest scheme to solve the multi-class classification problem. 

For features encoded using the ``bag-of-words'' model, we use the $\chi^2$ kernel to calculate the distance matrix. The optimal (C;g) parameters for learning Support Vector Machine (SVM) classifiers are found by conducting a grid search with five-fold cross validation on the original dataset. For features that are encoded with the Fisher vector, we use LibSVM with linear kernel. In this case, we perform a five-fold cross-validation to obtain the learning parameter C.

\subsection{Fusion Scheme}
Fusing information from different media seems to be a natural way to handle multimedia content. Fusing multi-modal information has been used for multimedia event detection in recent works such as \cite{natarajan2012multimodal,lan2012double,myers2014evaluating,oh2014multimedia}. The different types of multimedia data have their own characteristics, so it is also natural that they would have different fusion strategies \cite{xu2013feature}. Here, we chose to use late fusion with an average weighting scheme for all features \cite{snoek2005early}. This simple fusion strategy has demonstrated stable performance across many event detection collections as well as different set of features.
